{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b61a5200-e38f-413c-a1a5-45e5d49ea2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (2.1.2)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (0.20.1+cu124)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting pennylane\n",
      "  Downloading pennylane-0.43.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting rustworkx>=0.14.0 (from pennylane)\n",
      "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting autograd (from pennylane)\n",
      "  Downloading autograd-1.8.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting appdirs (from pennylane)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting autoray==0.8.0 (from pennylane)\n",
      "  Downloading autoray-0.8.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting cachetools (from pennylane)\n",
      "  Downloading cachetools-6.2.1-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting pennylane-lightning>=0.43 (from pennylane)\n",
      "  Downloading pennylane_lightning-0.43.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from pennylane) (2.32.3)\n",
      "Collecting tomlkit (from pennylane)\n",
      "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from pennylane) (24.1)\n",
      "Collecting diastatic-malt (from pennylane)\n",
      "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.43->pennylane)\n",
      "  Downloading scipy_openblas32-0.3.30.0.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: astunparse in /opt/conda/lib/python3.11/site-packages (from diastatic-malt->pennylane) (1.6.3)\n",
      "Collecting gast (from diastatic-malt->pennylane)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting termcolor (from diastatic-malt->pennylane)\n",
      "  Downloading termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->pennylane) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->pennylane) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->pennylane) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->pennylane) (2024.8.30)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse->diastatic-malt->pennylane) (0.44.0)\n",
      "Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pennylane-0.43.1-py3-none-any.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading autoray-0.8.0-py3-none-any.whl (934 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m934.3/934.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading pennylane_lightning-0.43.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading autograd-1.8.0-py3-none-any.whl (51 kB)\n",
      "Downloading cachetools-6.2.1-py3-none-any.whl (11 kB)\n",
      "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
      "Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Downloading scipy_openblas32-0.3.30.0.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Installing collected packages: appdirs, tomlkit, threadpoolctl, termcolor, scipy-openblas32, scipy, rustworkx, joblib, gast, cachetools, autoray, autograd, scikit-learn, pandas, diastatic-malt, pennylane-lightning, pennylane\n",
      "Successfully installed appdirs-1.4.4 autograd-1.8.0 autoray-0.8.0 cachetools-6.2.1 diastatic-malt-2.15.2 gast-0.6.0 joblib-1.5.2 pandas-2.3.3 pennylane-0.43.1 pennylane-lightning-0.43.0 rustworkx-0.17.1 scikit-learn-1.7.2 scipy-1.16.3 scipy-openblas32-0.3.30.0.7 termcolor-3.2.0 threadpoolctl-3.6.0 tomlkit-0.13.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install numpy pandas torch torchvision scikit-learn pennylane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74dcb06-d98b-4ffa-a5f3-2171411ab72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data_fashionmnist/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:19<00:00, 1.36MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_fashionmnist/FashionMNIST/raw/train-images-idx3-ubyte.gz to data_fashionmnist/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data_fashionmnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 207kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_fashionmnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data_fashionmnist/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data_fashionmnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.42M/4.42M [00:02<00:00, 1.91MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_fashionmnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data_fashionmnist/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data_fashionmnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 12.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_fashionmnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data_fashionmnist/FashionMNIST/raw\n",
      "\n",
      "[INFO] Fashion-MNIST subset (images): 8000 samples total (800/class)\n",
      "[teacher_full] Ep 01 TrainLoss 206.4146\n",
      "[teacher_full] Ep 01 | ValAcc 51.56% (best) | no_improve=0/8\n",
      "[teacher_full] Ep 02 TrainLoss 139.8017\n",
      "[teacher_full] Ep 02 | ValAcc 71.38% (best) | no_improve=0/8\n",
      "[teacher_full] Ep 03 TrainLoss 91.7418\n",
      "[teacher_full] Ep 03 | ValAcc 77.31% (best) | no_improve=0/8\n",
      "[teacher_full] Ep 04 TrainLoss 63.2160\n",
      "[teacher_full] Ep 04 | ValAcc 80.88% (best) | no_improve=0/8\n",
      "[teacher_full] Ep 05 TrainLoss 49.5949\n",
      "[teacher_full] Ep 05 | ValAcc 81.94% (best) | no_improve=0/8\n",
      "[teacher_full] Ep 06 TrainLoss 40.8572\n",
      "[teacher_full] Ep 06 | ValAcc 84.06% (best) | no_improve=0/8\n",
      "[teacher_full] Ep 07 TrainLoss 34.9777\n",
      "[teacher_full] Ep 07 | ValAcc 84.38% (best) | no_improve=0/8\n",
      "[teacher_full] Ep 08 TrainLoss 31.1814\n",
      "[teacher_full] Ep 08 | ValAcc 84.62% (best) | no_improve=0/8\n",
      "[teacher_full] Ep 09 TrainLoss 28.2829\n",
      "[teacher_full] Ep 09 | ValAcc 84.25%  | no_improve=1/8\n",
      "[teacher_full] Ep 10 TrainLoss 25.2948\n",
      "[teacher_full] Ep 10 | ValAcc 86.56% (best) | no_improve=0/8\n",
      "[teacher_full] Ep 11 TrainLoss 22.1300\n",
      "[teacher_full] Ep 11 | ValAcc 84.88%  | no_improve=1/8\n",
      "[teacher_full] Ep 12 TrainLoss 19.8122\n",
      "[teacher_full] Ep 12 | ValAcc 84.12%  | no_improve=2/8\n",
      "[teacher_full] Ep 13 TrainLoss 17.7957\n",
      "[teacher_full] Ep 13 | ValAcc 86.56%  | no_improve=3/8\n",
      "[teacher_full] Ep 14 TrainLoss 16.2892\n",
      "[teacher_full] Ep 14 | ValAcc 86.19%  | no_improve=4/8\n",
      "[teacher_full] Ep 15 TrainLoss 14.7533\n",
      "[teacher_full] Ep 15 | ValAcc 84.50%  | no_improve=5/8\n",
      "[teacher_full] Ep 16 TrainLoss 13.0532\n",
      "[teacher_full] Ep 16 | ValAcc 86.38%  | no_improve=6/8\n",
      "[teacher_full] Ep 17 TrainLoss 12.0361\n",
      "[teacher_full] Ep 17 | ValAcc 86.06%  | no_improve=7/8\n",
      "[teacher_full] Ep 18 TrainLoss 10.3560\n",
      "[teacher_full] Ep 18 | ValAcc 86.31%  | no_improve=8/8\n",
      "[teacher_full] Early stopping at epoch 18 (best @ 10).\n",
      "[teacher_full] Train finished in 16371.14s. Best val acc 86.56% @ epoch 10. Logs: ql_unlearning_outputs_fashionmnist/teacher_full_trainlog.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_371/2052057046.py:296: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  payload = torch.load(path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[oracle_subsetA] Ep 01 TrainLoss 196.0851\n",
      "[oracle_subsetA] Ep 01 | ValAcc 58.25% (best) | no_improve=0/8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Torch & sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Vision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# PennyLane\n",
    "import pennylane as qml\n",
    "from pennylane.qnn import TorchLayer\n",
    "\n",
    "# ----------------------------\n",
    "# Global config & utilities\n",
    "# ----------------------------\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "OUT_DIR = \"ql_unlearning_outputs_fashionmnist\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE_NAME = \"default.qubit\"     # change to \"default.mixed\" if enabling noise\n",
    "ENABLE_NOISE = False              # toggle noise simulation\n",
    "NOISE_P = 0.02                    # depolarizing prob if ENABLE_NOISE\n",
    "\n",
    "N_QUBITS = 10                     # number of angles for quantum layer\n",
    "N_CLASSES = 10                    # Fashion-MNIST has 10 classes\n",
    "BATCH = 64\n",
    "LR = 1e-3\n",
    "EPOCHS_TEACHER = 60\n",
    "\n",
    "# >>> select input frontend:\n",
    "USE_CNN_FRONTEND = True  # True = CNN→QLayer, False = PCA→QLayer\n",
    "\n",
    "# ----------------------------\n",
    "# Data loading (Fashion-MNIST + PCA)  -- vector version\n",
    "# ----------------------------\n",
    "def load_data(n_components=N_QUBITS, samples_per_class=100):\n",
    "    \"\"\"\n",
    "    Load Fashion-MNIST (train+test), sample balanced `samples_per_class` per class,\n",
    "    reduce with PCA to n_components, scale to [-π, π].\n",
    "    Return X (N x n_components), y (N,).\n",
    "    \"\"\"\n",
    "    tfm = transforms.Compose([transforms.ToTensor()])\n",
    "    train_ds = datasets.FashionMNIST(root=\"data_fashionmnist\", train=True,  download=True, transform=tfm)\n",
    "    test_ds  = datasets.FashionMNIST(root=\"data_fashionmnist\", train=False, download=True, transform=tfm)\n",
    "\n",
    "    # data is [N, 28, 28]\n",
    "    X_train = train_ds.data.numpy().reshape(-1, 28*28) / 255.0\n",
    "    y_train = train_ds.targets.numpy().astype(np.int64)\n",
    "    X_test  = test_ds.data.numpy().reshape(-1, 28*28) / 255.0\n",
    "    y_test  = test_ds.targets.numpy().astype(np.int64)\n",
    "\n",
    "    X = np.vstack([X_train, X_test])\n",
    "    y = np.concatenate([y_train, y_test])\n",
    "\n",
    "    # balanced sampling\n",
    "    X_sel, y_sel = [], []\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    for c in range(N_CLASSES):\n",
    "        idx_c = np.where(y == c)[0]\n",
    "        chosen = rng.choice(idx_c, size=min(samples_per_class, len(idx_c)), replace=False)\n",
    "        X_sel.append(X[chosen])\n",
    "        y_sel.append(y[chosen])\n",
    "    X_sel = np.vstack(X_sel)\n",
    "    y_sel = np.concatenate(y_sel)\n",
    "\n",
    "    # PCA and scaling for angle encoding\n",
    "    pca = PCA(n_components=n_components, random_state=SEED)\n",
    "    X_red = pca.fit_transform(X_sel)\n",
    "    scaler = MinMaxScaler(feature_range=(-np.pi, np.pi))\n",
    "    X_enc = scaler.fit_transform(X_red)\n",
    "\n",
    "    print(f\"[INFO] Fashion-MNIST subset (PCA): {len(X_enc)} samples total ({samples_per_class}/class)\")\n",
    "    return X_enc, y_sel\n",
    "\n",
    "# ----------------------------\n",
    "# Data loading (Fashion-MNIST images) -- for the CNN hybrid\n",
    "# ----------------------------\n",
    "def load_data_cnn(samples_per_class=100):\n",
    "    \"\"\"\n",
    "    Load Fashion-MNIST as images (1×28×28), sample balanced `samples_per_class` per class.\n",
    "    Return X_imgs [N,1,28,28] in [0,1], y [N].\n",
    "    \"\"\"\n",
    "    tfm = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    train_ds = datasets.FashionMNIST(root=\"data_fashionmnist\", train=True,  download=True, transform=tfm)\n",
    "    test_ds  = datasets.FashionMNIST(root=\"data_fashionmnist\", train=False, download=True, transform=tfm)\n",
    "\n",
    "    # train_ds.data/test_ds.data: [N,28,28], grayscale\n",
    "    X = torch.cat([train_ds.data, test_ds.data], dim=0).float() / 255.0  # [N,28,28]\n",
    "    y = torch.cat([train_ds.targets, test_ds.targets], dim=0).long()     # [N]\n",
    "\n",
    "    X = X.unsqueeze(1)  # [N,1,28,28]\n",
    "\n",
    "    # balanced sampling\n",
    "    X_sel, y_sel = [], []\n",
    "    g = torch.Generator().manual_seed(SEED)\n",
    "    for c in range(N_CLASSES):\n",
    "        idx = (y == c).nonzero(as_tuple=True)[0]\n",
    "        perm = torch.randperm(len(idx), generator=g)\n",
    "        chosen = idx[perm[:samples_per_class]]\n",
    "        X_sel.append(X[chosen])\n",
    "        y_sel.append(y[chosen])\n",
    "\n",
    "    X_sel = torch.cat(X_sel, dim=0)  # [N,1,28,28]\n",
    "    y_sel = torch.cat(y_sel, dim=0)\n",
    "\n",
    "    print(f\"[INFO] Fashion-MNIST subset (images): {len(X_sel)} samples total ({samples_per_class}/class)\")\n",
    "    return X_sel.numpy(), y_sel.numpy()\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers for tensors & loaders\n",
    "# ----------------------------\n",
    "def to_tensors(X, y=None):\n",
    "    X_t = torch.tensor(X, dtype=torch.float32)\n",
    "    if y is None:\n",
    "        return X_t\n",
    "    y_t = torch.tensor(y, dtype=torch.long)\n",
    "    return X_t, y_t\n",
    "\n",
    "def make_loaders(X_train, y_train, X_test, y_test, batch=BATCH):\n",
    "    train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                             torch.tensor(y_train, dtype=torch.long))\n",
    "    test_ds = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                            torch.tensor(y_test, dtype=torch.long))\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def split_scenarios(X, y, subset_rate=0.02):\n",
    "    \"\"\"\n",
    "    Split into:\n",
    "      - train/test (80/20, stratified)\n",
    "      - Scenario A: 'forget' = random subset (2%) of train\n",
    "      - Scenario B: 'forget' = entire class 0 from train\n",
    "    \"\"\"\n",
    "    X_tr_all, X_te, y_tr_all, y_te = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    "    )\n",
    "    # A: small forget subset\n",
    "    X_dr_A, X_df_A, y_dr_A, y_df_A = train_test_split(\n",
    "        X_tr_all, y_tr_all, test_size=subset_rate, random_state=SEED, stratify=y_tr_all\n",
    "    )\n",
    "    # B: full class 0 as forget\n",
    "    mask_class0 = (y_tr_all == 0)\n",
    "    X_df_B = X_tr_all[mask_class0]\n",
    "    y_df_B = y_tr_all[mask_class0]\n",
    "    X_dr_B = X_tr_all[~mask_class0]\n",
    "    y_dr_B = y_tr_all[~mask_class0]\n",
    "\n",
    "    return (\n",
    "        (X_tr_all, y_tr_all, X_te, y_te),\n",
    "        (X_dr_A, y_dr_A, X_df_A, y_df_A),\n",
    "        (X_dr_B, y_dr_B, X_df_B, y_df_B),\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# Quantum model definition\n",
    "# ----------------------------\n",
    "def make_device():\n",
    "    if ENABLE_NOISE:\n",
    "        dev = qml.device(\"default.mixed\", wires=N_QUBITS, shots=None)\n",
    "    else:\n",
    "        dev = qml.device(DEVICE_NAME, wires=N_QUBITS, shots=None)\n",
    "    return dev\n",
    "\n",
    "def noise_block():\n",
    "    if ENABLE_NOISE:\n",
    "        for w in range(N_QUBITS):\n",
    "            qml.DepolarizingChannel(NOISE_P, wires=w)\n",
    "\n",
    "def qnode_def(dev):\n",
    "    @qml.qnode(dev, interface=\"torch\")\n",
    "    def qnode(inputs, weights):\n",
    "        # inputs: 1D tensor with N_QUBITS angles in [-π, π]\n",
    "        assert inputs.shape[0] == N_QUBITS, f\"Expected {N_QUBITS} features, got {inputs.shape[0]}\"\n",
    "        # Angle encoding with RX, RY, RZ\n",
    "        for w, x in enumerate(inputs):\n",
    "            qml.RX(x, wires=w)\n",
    "            qml.RY(x, wires=w)\n",
    "            qml.RZ(x, wires=w)\n",
    "        noise_block()\n",
    "        # L variational layers + CZ ring\n",
    "        L = weights.shape[0]\n",
    "        for l in range(L):\n",
    "            for w in range(N_QUBITS):\n",
    "                qml.RX(weights[l, 0, w], wires=w)\n",
    "                qml.RY(weights[l, 1, w], wires=w)\n",
    "                qml.RZ(weights[l, 2, w], wires=w)\n",
    "            for w in range(N_QUBITS - 1):\n",
    "                qml.CZ(wires=[w, w + 1])\n",
    "            qml.CZ(wires=[N_QUBITS - 1, 0])\n",
    "            noise_block()\n",
    "        # Z expectations -> N_QUBITS real outputs\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(N_QUBITS)]\n",
    "    return qnode\n",
    "\n",
    "# ----------------------------\n",
    "# Models: PCA-hybrid & CNN-hybrid\n",
    "# ----------------------------\n",
    "class HybridQCNN(nn.Module):\n",
    "    \"\"\" Vector PCA → QLayer → 2 dense layers \"\"\"\n",
    "    def __init__(self, layers=2):\n",
    "        super().__init__()\n",
    "        dev = make_device()\n",
    "        qnode = qnode_def(dev)\n",
    "        weight_shapes = {\"weights\": (layers, 3, N_QUBITS)}\n",
    "        self.q_layer = TorchLayer(qnode, weight_shapes)\n",
    "        self.fc1 = nn.Linear(N_QUBITS, 16)\n",
    "        self.fc2 = nn.Linear(16, N_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, N_QUBITS]\n",
    "        outs = [self.q_layer(sample) for sample in x]\n",
    "        q_out = torch.stack(outs)   # [B,N_QUBITS]\n",
    "        q_out = torch.relu(self.fc1(q_out))\n",
    "        logits = self.fc2(q_out)\n",
    "        return logits\n",
    "\n",
    "class HybridQConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Fashion-MNIST: small CNN (1×28×28) → projection to N_QUBITS (angles) → quantum layer → classical head\n",
    "    \"\"\"\n",
    "    def __init__(self, layers=2):\n",
    "        super().__init__()\n",
    "        # 1) CNN frontend (1×28×28)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # 1×28×28 → 16×28×28\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),                                       # → 16×14×14\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1), # → 32×14×14\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),                                       # → 32×7×7\n",
    "        )\n",
    "        self.to_angles = nn.Sequential(\n",
    "            nn.Flatten(),                           # 32*7*7 = 1568\n",
    "            nn.Linear(32*7*7, N_QUBITS)             # projection to N_QUBITS\n",
    "        )\n",
    "\n",
    "        # 2) Quantum layer\n",
    "        dev = make_device()\n",
    "        qnode = qnode_def(dev)\n",
    "        weight_shapes = {\"weights\": (layers, 3, N_QUBITS)}\n",
    "        self.q_layer = TorchLayer(qnode, weight_shapes)\n",
    "\n",
    "        # 3) Classical head\n",
    "        self.fc1 = nn.Linear(N_QUBITS, 32)\n",
    "        self.fc2 = nn.Linear(32, N_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B,1,28,28]\n",
    "        \"\"\"\n",
    "        feat = self.cnn(x)                          # [B,32,7,7]\n",
    "        angles = self.to_angles(feat)               # [B,N_QUBITS]\n",
    "        angles = torch.tanh(angles) * math.pi       # map to [-π, π]\n",
    "\n",
    "        outs = [self.q_layer(sample) for sample in angles]  # list of [N_QUBITS]\n",
    "        q_out = torch.stack(outs)                            # [B,N_QUBITS]\n",
    "\n",
    "        x = torch.relu(self.fc1(q_out))\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n",
    "\n",
    "# ----------------------------\n",
    "# Training & evaluation utils\n",
    "# ----------------------------\n",
    "def save_model(model, path, extra: Dict = None):\n",
    "    payload = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"extra\": extra or {}\n",
    "    }\n",
    "    torch.save(payload, path)\n",
    "\n",
    "def load_model(path, layers=2):\n",
    "    model = HybridQConv(layers=layers) if USE_CNN_FRONTEND else HybridQCNN(layers=layers)\n",
    "    payload = torch.load(path, map_location=\"cpu\")\n",
    "    model.load_state_dict(payload[\"model_state_dict\"])\n",
    "    return model, payload.get(\"extra\", {})\n",
    "\n",
    "def softmax_probs(logits: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.softmax(logits, dim=1)\n",
    "\n",
    "def evaluate_acc(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb).argmax(1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    return correct / total if total else 0.0\n",
    "\n",
    "def get_probs(model, loader) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    model.eval()\n",
    "    probs_list, y_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            probs = softmax_probs(model(xb)).cpu().numpy()\n",
    "            probs_list.append(probs)\n",
    "            y_list.append(yb.cpu().numpy())\n",
    "    return np.vstack(probs_list), np.concatenate(y_list)\n",
    "\n",
    "def kl_divergence(P: np.ndarray, Q: np.ndarray, eps=1e-8) -> float:\n",
    "    P = np.clip(P, eps, 1)\n",
    "    Q = np.clip(Q, eps, 1)\n",
    "    return float(np.mean(np.sum(P * (np.log(P) - np.log(Q)), axis=1)))\n",
    "\n",
    "def js_divergence(P: np.ndarray, Q: np.ndarray, eps=1e-8) -> float:\n",
    "    M = 0.5 * (P + Q)\n",
    "    def _kl(A, B):\n",
    "        A = np.clip(A, eps, 1); B = np.clip(B, eps, 1)\n",
    "        return np.sum(A * (np.log(A) - np.log(B)), axis=1)\n",
    "    return float(0.5 * np.mean(_kl(P, M)) + 0.5 * np.mean(_kl(Q, M)))\n",
    "\n",
    "def agreement_rate(model_a, model_b, loader) -> float:\n",
    "    model_a.eval(); model_b.eval()\n",
    "    pa, pb = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in loader:\n",
    "            pa.append(model_a(xb).argmax(1).cpu().numpy())\n",
    "            pb.append(model_b(xb).argmax(1).cpu().numpy())\n",
    "    pa = np.concatenate(pa); pb = np.concatenate(pb)\n",
    "    return float(np.mean(pa == pb))\n",
    "\n",
    "def mia_auc_confidence(model, members_loader, nonmembers_loader) -> float:\n",
    "    def scores(loader):\n",
    "        s = []\n",
    "        with torch.no_grad():\n",
    "            for xb, _ in loader:\n",
    "                probs = softmax_probs(model(xb)).cpu().numpy()\n",
    "                s.extend(np.max(probs, axis=1))\n",
    "        return np.array(s)\n",
    "\n",
    "    s_members = scores(members_loader)\n",
    "    s_nonmembers = scores(nonmembers_loader)\n",
    "    labels = np.concatenate([np.ones_like(s_members), np.zeros_like(s_nonmembers)])\n",
    "    scores_all = np.concatenate([s_members, s_nonmembers])\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, scores_all)\n",
    "    except Exception:\n",
    "        auc = float(\"nan\")\n",
    "    return float(auc)\n",
    "\n",
    "# =========================\n",
    "# Early Stopping utilities\n",
    "# =========================\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=6, min_delta=0.0, best_is_max=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_is_max = best_is_max\n",
    "        self.best_val = None\n",
    "        self.best_epoch = 0\n",
    "        self.epochs_no_improve = 0\n",
    "        self.best_state = None\n",
    "\n",
    "    def _is_better(self, val):\n",
    "        if self.best_val is None:\n",
    "            return True\n",
    "        if self.best_is_max:\n",
    "            return (val - self.best_val) > self.min_delta\n",
    "        else:\n",
    "            return (self.best_val - val) > self.min_delta\n",
    "\n",
    "    def step(self, val, model, epoch):\n",
    "        if self._is_better(val):\n",
    "            self.best_val = val\n",
    "            self.best_epoch = epoch\n",
    "            self.epochs_no_improve = 0\n",
    "            self.best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            improved = True\n",
    "        else:\n",
    "            self.epochs_no_improve += 1\n",
    "            improved = False\n",
    "        stop = self.epochs_no_improve >= self.patience\n",
    "        return stop, improved\n",
    "\n",
    "def _save_json(path, obj):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def train_with_es(\n",
    "    model,\n",
    "    epochs,\n",
    "    eval_loader,\n",
    "    tag,\n",
    "    train_epoch_fn,  # closure: train one epoch\n",
    "    best_is_max=True,\n",
    "    patience=6,\n",
    "    min_delta=0.0,\n",
    "    save_every_epoch=True,\n",
    "):\n",
    "    t0 = time.perf_counter()\n",
    "    stopper = EarlyStopper(patience=patience, min_delta=min_delta, best_is_max=best_is_max)\n",
    "\n",
    "    best_path = os.path.join(OUT_DIR, f\"{tag}_best.pth\")\n",
    "    per_epoch_paths = []\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        train_epoch_fn(ep)\n",
    "\n",
    "        val_acc = evaluate_acc(model, eval_loader)\n",
    "\n",
    "        if save_every_epoch:\n",
    "            ep_path = os.path.join(OUT_DIR, f\"{tag}_ep{ep}.pth\")\n",
    "            save_model(model, ep_path, extra={\"epoch\": ep, \"val_acc\": val_acc, \"tag\": tag})\n",
    "            per_epoch_paths.append(ep_path)\n",
    "\n",
    "        stop, improved = stopper.step(val_acc, model, ep)\n",
    "        print(f\"[{tag}] Ep {ep:02d} | ValAcc {val_acc*100:.2f}% \"\n",
    "              f\"{'(best)' if improved else ''} | no_improve={stopper.epochs_no_improve}/{patience}\")\n",
    "        if stop:\n",
    "            print(f\"[{tag}] Early stopping at epoch {ep} (best @ {stopper.best_epoch}).\")\n",
    "            break\n",
    "\n",
    "    if stopper.best_state is not None:\n",
    "        model.load_state_dict(stopper.best_state)\n",
    "        save_model(model, best_path, extra={\"epoch\": stopper.best_epoch, \"best_val_acc\": stopper.best_val, \"tag\": tag})\n",
    "\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    log = {\n",
    "        \"tag\": tag,\n",
    "        \"epochs_requested\": epochs,\n",
    "        \"epochs_ran\": ep,\n",
    "        \"stopped_early\": (ep < epochs),\n",
    "        \"best_epoch\": stopper.best_epoch,\n",
    "        \"best_val_acc\": stopper.best_val,\n",
    "        \"patience\": patience,\n",
    "        \"min_delta\": min_delta,\n",
    "        \"elapsed_sec\": elapsed,\n",
    "        \"per_epoch_checkpoints\": per_epoch_paths,\n",
    "        \"best_checkpoint\": best_path,\n",
    "    }\n",
    "    _save_json(os.path.join(OUT_DIR, f\"{tag}_trainlog.json\"), log)\n",
    "    print(f\"[{tag}] Train finished in {elapsed:.2f}s. Best val acc {100*(stopper.best_val or 0):.2f}% @ epoch {stopper.best_epoch}. \"\n",
    "          f\"Logs: {os.path.join(OUT_DIR, f'{tag}_trainlog.json')}\")\n",
    "    return model, log\n",
    "\n",
    "# ----------------------------\n",
    "# Teacher training (with ES)\n",
    "# ----------------------------\n",
    "def train_teacher(X_train, y_train, X_test, y_test, layers=2, epochs=EPOCHS_TEACHER, lr=LR, tag=\"teacher\",\n",
    "                  patience=6, min_delta=0.0):\n",
    "    model = HybridQConv(layers=layers) if USE_CNN_FRONTEND else HybridQCNN(layers=layers)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loader, eval_loader = make_loaders(X_train, y_train, X_test, y_test, batch=BATCH)\n",
    "\n",
    "    def _one_epoch(ep):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            opt.zero_grad()\n",
    "            loss = criterion(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"[{tag}] Ep {ep:02d} TrainLoss {total_loss:.4f}\")\n",
    "\n",
    "    model, _ = train_with_es(\n",
    "        model=model,\n",
    "        epochs=epochs,\n",
    "        eval_loader=eval_loader,\n",
    "        tag=tag,\n",
    "        train_epoch_fn=_one_epoch,\n",
    "        best_is_max=True,\n",
    "        patience=patience,\n",
    "        min_delta=min_delta,\n",
    "        save_every_epoch=True,\n",
    "    )\n",
    "\n",
    "    best_path = os.path.join(OUT_DIR, f\"{tag}_best.pth\")\n",
    "    model_best, _ = load_model(best_path, layers=layers)\n",
    "    return model_best, (train_loader, eval_loader), best_path\n",
    "\n",
    "# ----------------------------\n",
    "# Layer-group utility (CF-k)\n",
    "# ----------------------------\n",
    "def get_layer_groups_for_freeze(model):\n",
    "    \"\"\"\n",
    "    Return ordered list of 'layer groups' for CF-k, from input to output.\n",
    "    \"\"\"\n",
    "    if isinstance(model, HybridQConv):\n",
    "        return [model.cnn, model.to_angles, model.q_layer, model.fc1, model.fc2]\n",
    "    else:\n",
    "        return [model.q_layer, model.fc1, model.fc2]\n",
    "\n",
    "# ----------------------------\n",
    "# Unlearning Methods (with ES)\n",
    "# ----------------------------\n",
    "def method_gradient_ascent(model, forget_loader, epochs=10, lr=LR, tag=\"GA\",\n",
    "                           eval_loader=None, patience=6, min_delta=0.0):\n",
    "    model = copy.deepcopy(model)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    def _one_epoch(ep):\n",
    "        model.train()\n",
    "        for xb, yb in forget_loader:\n",
    "            opt.zero_grad()\n",
    "            loss = -criterion(model(xb), yb)  # ascent on forget\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    model, _ = train_with_es(model, epochs, eval_loader, tag, _one_epoch,\n",
    "                             best_is_max=True, patience=patience, min_delta=min_delta)\n",
    "    return model\n",
    "\n",
    "def compute_fisher(model, loader, criterion):\n",
    "    model.eval()\n",
    "    fisher = {n: torch.zeros_like(p, dtype=torch.float32) for n, p in model.named_parameters() if p.requires_grad}\n",
    "    for xb, yb in loader:\n",
    "        model.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        for (n, p) in model.named_parameters():\n",
    "            if p.grad is not None and p.requires_grad:\n",
    "                fisher[n] += (p.grad.detach() ** 2)\n",
    "    for n in fisher:\n",
    "        fisher[n] /= len(loader)\n",
    "    return fisher\n",
    "\n",
    "def method_fisher_unlearning(model, forget_loader, lambda_ewc=10.0, epochs=10, lr=LR, tag=\"Fisher\",\n",
    "                             eval_loader=None, patience=6, min_delta=0.0):\n",
    "    model = copy.deepcopy(model)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    old_params = {n: p.clone().detach() for n, p in model.named_parameters() if p.requires_grad}\n",
    "    fisher = compute_fisher(model, forget_loader, criterion)\n",
    "\n",
    "    def _one_epoch(ep):\n",
    "        model.train()\n",
    "        for xb, yb in forget_loader:\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = -criterion(logits, yb)\n",
    "            for (n, p) in model.named_parameters():\n",
    "                if p.requires_grad:\n",
    "                    loss = loss + (lambda_ewc / 2.0) * torch.sum(fisher[n] * (p - old_params[n]) ** 2)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    model, _ = train_with_es(model, epochs, eval_loader, tag, _one_epoch,\n",
    "                             best_is_max=True, patience=patience, min_delta=min_delta)\n",
    "    return model\n",
    "\n",
    "def method_neggrad_plus(model, retain_loader, forget_loader, epochs=10, lr=LR, alpha=1.0, tag=\"NegGradPlus\",\n",
    "                        eval_loader=None, patience=6, min_delta=0.0):\n",
    "    model = copy.deepcopy(model)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    def _one_epoch(ep):\n",
    "        model.train()\n",
    "        for (xb_r, yb_r), (xb_f, yb_f) in zip(retain_loader, forget_loader):\n",
    "            opt.zero_grad()\n",
    "            out_r = model(xb_r)\n",
    "            out_f = model(xb_f)\n",
    "            loss = criterion(out_r, yb_r) - alpha * criterion(out_f, yb_f)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    model, _ = train_with_es(model, epochs, eval_loader, tag, _one_epoch,\n",
    "                             best_is_max=True, patience=patience, min_delta=min_delta)\n",
    "    return model\n",
    "\n",
    "def method_cf_k(model, forget_loader, k=1, epochs=10, lr=LR, tag=\"CFk\",\n",
    "                eval_loader=None, patience=6, min_delta=0.0):\n",
    "    model = copy.deepcopy(model)\n",
    "    groups = get_layer_groups_for_freeze(model)\n",
    "    for i, grp in enumerate(groups):\n",
    "        for p in grp.parameters() if hasattr(grp, \"parameters\") else []:\n",
    "            p.requires_grad = not (i < k)\n",
    "\n",
    "    opt = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def _one_epoch(ep):\n",
    "        model.train()\n",
    "        for xb, yb in forget_loader:\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = -criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    model, _ = train_with_es(model, epochs, eval_loader, tag, _one_epoch,\n",
    "                             best_is_max=True, patience=patience, min_delta=min_delta)\n",
    "    return model\n",
    "\n",
    "def method_eu_k(model, retain_loader, k=1, epochs=10, lr=LR, tag=\"EUk\",\n",
    "                eval_loader=None, patience=6, min_delta=0.0):\n",
    "    \"\"\"\n",
    "    Reinitialize last k *classical* layers and retrain on Dr.\n",
    "    For both models we target the classical head: fc2, then fc1.\n",
    "    \"\"\"\n",
    "    model = copy.deepcopy(model)\n",
    "    # Reinit tail layers (classical head)\n",
    "    if k >= 1:\n",
    "        if hasattr(model, \"fc2\"):\n",
    "            model.fc2 = nn.Linear(model.fc1.out_features, N_CLASSES)\n",
    "    if k >= 2:\n",
    "        if hasattr(model, \"fc1\"):\n",
    "            model.fc1 = nn.Linear(N_QUBITS, model.fc2.in_features if hasattr(model.fc2, \"in_features\") else 32)\n",
    "        if hasattr(model, \"fc2\"):\n",
    "            model.fc2 = nn.Linear(model.fc1.out_features, N_CLASSES)\n",
    "\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def _one_epoch(ep):\n",
    "        model.train()\n",
    "        for xb, yb in retain_loader:\n",
    "            opt.zero_grad()\n",
    "            loss = criterion(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    model, _ = train_with_es(model, epochs, eval_loader, tag, _one_epoch,\n",
    "                             best_is_max=True, patience=patience, min_delta=min_delta)\n",
    "    return model\n",
    "\n",
    "# === Certified Unlearning (noisy fine-tune on Dr)\n",
    "def method_certified_unlearning(model, retain_loader, epochs=20, lr=LR,\n",
    "                                sigma=0.05, clip_norm=1.0, tag=\"Certified\",\n",
    "                                eval_loader=None, patience=6, min_delta=0.0):\n",
    "    model = copy.deepcopy(model)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    def _one_epoch(ep):\n",
    "        model.train()\n",
    "        for xb, yb in retain_loader:\n",
    "            opt.zero_grad()\n",
    "            loss = criterion(model(xb), yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm)\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    p.grad += sigma * torch.randn_like(p.grad)\n",
    "            opt.step()\n",
    "\n",
    "    model, _ = train_with_es(model, epochs, eval_loader, tag, _one_epoch,\n",
    "                             best_is_max=True, patience=patience, min_delta=min_delta)\n",
    "    return model\n",
    "\n",
    "# === Q-MUL-inspired\n",
    "def build_similar_label_map(teacher_model, data_X, data_y) -> Dict[int, int]:\n",
    "    teacher_model.eval()\n",
    "    X_t = torch.tensor(data_X, dtype=torch.float32)\n",
    "    y_t = torch.tensor(data_y, dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        logits = teacher_model(X_t).cpu().numpy()\n",
    "\n",
    "    class_means = []\n",
    "    y_np = y_t.numpy()\n",
    "    for c in range(N_CLASSES):\n",
    "        mask = (y_np == c)\n",
    "        if np.sum(mask) == 0:\n",
    "            class_means.append(np.zeros((N_CLASSES,), dtype=np.float32))\n",
    "        else:\n",
    "            class_means.append(logits[mask].mean(axis=0))\n",
    "    class_means = np.stack(class_means, axis=0)\n",
    "\n",
    "    def cos(a, b):\n",
    "        na = np.linalg.norm(a) + 1e-12\n",
    "        nb = np.linalg.norm(b) + 1e-12\n",
    "        return float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "    mapping = {}\n",
    "    for c in range(N_CLASSES):\n",
    "        best = None\n",
    "        for d in range(N_CLASSES):\n",
    "            if d == c:\n",
    "                continue\n",
    "            s = cos(class_means[c], class_means[d])\n",
    "            if best is None or s > best[0]:\n",
    "                best = (s, d)\n",
    "        mapping[c] = best[1] if best else c\n",
    "    return mapping\n",
    "\n",
    "def method_qmul(teacher_model, retain_loader, forget_loader, epochs=20,\n",
    "                lr=LR, alpha=1.0, tag=\"Q-MUL\",\n",
    "                eval_loader=None, patience=6, min_delta=0.0):\n",
    "    model = copy.deepcopy(teacher_model)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    def stack_loader(loader):\n",
    "        Xs, Ys = [], []\n",
    "        for xb, yb in loader:\n",
    "            Xs.append(xb); Ys.append(yb)\n",
    "        return torch.cat(Xs).numpy(), torch.cat(Ys).numpy()\n",
    "    Xr, yr = stack_loader(retain_loader)\n",
    "    sim_map = build_similar_label_map(teacher_model, Xr, yr)\n",
    "\n",
    "    eps = 1e-12\n",
    "    def _one_epoch(ep):\n",
    "        model.train()\n",
    "        for (xb_r, yb_r), (xb_f, yb_f) in zip(retain_loader, forget_loader):\n",
    "            # similar-label substitution for forget batch\n",
    "            yb_f_sim = yb_f.clone()\n",
    "            for i in range(yb_f_sim.shape[0]):\n",
    "                yb_f_sim[i] = sim_map[int(yb_f_sim[i].item())]\n",
    "\n",
    "            # gradient norms\n",
    "            opt.zero_grad()\n",
    "            Lr = criterion(model(xb_r), yb_r)\n",
    "            Lr.backward(retain_graph=True)\n",
    "            gnorm_r = torch.sqrt(sum([(p.grad.detach()**2).sum()\n",
    "                                      for p in model.parameters() if p.grad is not None]) + eps).item()\n",
    "\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            Lf = criterion(model(xb_f), yb_f_sim)\n",
    "            Lf.backward(retain_graph=True)\n",
    "            gnorm_f = torch.sqrt(sum([(p.grad.detach()**2).sum()\n",
    "                                      for p in model.parameters() if p.grad is not None]) + eps).item()\n",
    "\n",
    "            w = (gnorm_r / (gnorm_f + eps)) * alpha\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            loss = Lr - w * Lf\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    model, _ = train_with_es(model, epochs, eval_loader, tag, _one_epoch,\n",
    "                             best_is_max=True, patience=patience, min_delta=min_delta)\n",
    "    return model\n",
    "\n",
    "# ------------- SCRUB -------------\n",
    "def kl_torch(p_teacher: torch.Tensor, p_student: torch.Tensor, eps=1e-8):\n",
    "    p_teacher = torch.clamp(p_teacher, eps, 1.0)\n",
    "    p_student = torch.clamp(p_student, eps, 1.0)\n",
    "    return torch.sum(p_teacher * (torch.log(p_teacher) - torch.log(p_student)), dim=1).mean()\n",
    "\n",
    "def method_scrub(model_teacher, retain_loader, forget_loader, epochs=50, lr=LR, lam_r=1.0, lam_f=1.0, tag=\"SCRUB\",\n",
    "                 rewind=False, eval_loader=None, patience=6, min_delta=0.0):\n",
    "    student = copy.deepcopy(model_teacher)\n",
    "    opt = optim.Adam(student.parameters(), lr=lr)\n",
    "\n",
    "    best_score = -1e9\n",
    "    best_path_rewind = os.path.join(OUT_DIR, f\"{tag}_rewind_best.pth\")\n",
    "\n",
    "    def _one_epoch(ep):\n",
    "        nonlocal best_score\n",
    "        student.train()\n",
    "        for (xb_r, yb_r), (xb_f, yb_f) in zip(retain_loader, forget_loader):\n",
    "            opt.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                pt_r = softmax_probs(model_teacher(xb_r))\n",
    "                pt_f = softmax_probs(model_teacher(xb_f))\n",
    "            ps_r = softmax_probs(student(xb_r))\n",
    "            ps_f = softmax_probs(student(xb_f))\n",
    "\n",
    "            L_obey = kl_torch(pt_r, ps_r)       # minimize on retain\n",
    "            L_disobey = kl_torch(pt_f, ps_f)    # maximize on forget\n",
    "\n",
    "            loss = lam_r * L_obey - lam_f * L_disobey\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        if rewind:\n",
    "            score = (-L_obey.item()) + (L_disobey.item())\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                save_model(student, best_path_rewind, extra={\"epoch\": ep, \"score\": score, \"tag\": tag})\n",
    "\n",
    "    student, _ = train_with_es(student, epochs, eval_loader, tag, _one_epoch,\n",
    "                               best_is_max=True, patience=patience, min_delta=min_delta)\n",
    "    # If you want to reload the best rewind checkpoint, uncomment:\n",
    "    # if rewind and os.path.exists(best_path_rewind):\n",
    "    #     student, _ = load_model(best_path_rewind, layers=2)\n",
    "    return student\n",
    "\n",
    "# ---------- LCA ----------\n",
    "from itertools import cycle\n",
    "\n",
    "def make_label_complement_loader(forget_loader, n_classes=N_CLASSES, batch=BATCH, shuffle=True):\n",
    "    Xs, Ys = [], []\n",
    "    for xb, yb in forget_loader:\n",
    "        Xs.append(xb)\n",
    "        Ys.append(yb)\n",
    "    if len(Xs) == 0:\n",
    "        raise ValueError(\"forget_loader is empty — cannot build label-complement loader.\")\n",
    "    X = torch.cat(Xs)                          # [N, ...]\n",
    "    y = torch.cat(Ys)                          # [N]\n",
    "\n",
    "    N = X.shape[0]\n",
    "    all_classes = torch.arange(n_classes).unsqueeze(0).repeat(N, 1)   # [N, C]\n",
    "    mask = (all_classes != y.unsqueeze(1))                            # [N, C]\n",
    "    y_comp = all_classes[mask].reshape(-1)                            # [N*(C-1)]\n",
    "\n",
    "    reps = n_classes - 1\n",
    "    X_rep = X.unsqueeze(1).repeat(1, reps, *([1] * (X.dim()-1))).reshape(-1, *X.shape[1:])\n",
    "\n",
    "    ds = TensorDataset(X_rep.float(), y_comp.long())\n",
    "    return DataLoader(ds, batch_size=batch, shuffle=shuffle)\n",
    "\n",
    "def method_label_complement_augmentation(teacher_model,\n",
    "                                         retain_loader,\n",
    "                                         forget_loader,\n",
    "                                         epochs=10,\n",
    "                                         lr=LR,\n",
    "                                         beta=1.0,\n",
    "                                         tag=\"LCA\",\n",
    "                                         eval_loader=None, patience=6, min_delta=0.0):\n",
    "    model = copy.deepcopy(teacher_model)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    comp_loader = make_label_complement_loader(forget_loader, n_classes=N_CLASSES, batch=BATCH, shuffle=True)\n",
    "\n",
    "    def _one_epoch(ep):\n",
    "        model.train()\n",
    "        comp_iter = iter(comp_loader)\n",
    "        for xb_r, yb_r in retain_loader:\n",
    "            try:\n",
    "                xb_c, yb_c = next(comp_iter)\n",
    "            except StopIteration:\n",
    "                comp_iter = iter(comp_loader)\n",
    "                xb_c, yb_c = next(comp_iter)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            Lr = criterion(model(xb_r), yb_r)\n",
    "            Lc = criterion(model(xb_c), yb_c)\n",
    "            loss = Lr + beta * Lc\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    model, _ = train_with_es(model, epochs, eval_loader, tag, _one_epoch,\n",
    "                             best_is_max=True, patience=patience, min_delta=min_delta)\n",
    "    return model\n",
    "\n",
    "# ---------- ADV-UNIFORM ----------\n",
    "def fgsm_on_inputs(model, xb, eps=0.1, y_target_uniform=True):\n",
    "    xb = xb.clone().detach().requires_grad_(True)\n",
    "    logits = model(xb)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    if y_target_uniform:\n",
    "        U = torch.full_like(probs, 1.0 / probs.shape[1])\n",
    "        loss = torch.sum(probs * (torch.log(probs + 1e-8) - torch.log(U + 1e-8)), dim=1).mean()\n",
    "    else:\n",
    "        loss = -torch.distributions.Categorical(probs).entropy().mean()\n",
    "    loss.backward()\n",
    "    x_adv = xb + eps * xb.grad.sign()\n",
    "    # For PCA: clamp to [-π,π]; for images: clamp to [0,1].\n",
    "    if USE_CNN_FRONTEND:\n",
    "        return torch.clamp(x_adv, 0.0, 1.0).detach()\n",
    "    else:\n",
    "        return torch.clamp(x_adv, -math.pi, math.pi).detach()\n",
    "\n",
    "def method_adv_uniform(teacher_model, retain_loader, forget_loader,\n",
    "                       epochs=10, lr=LR, lam=1.0, eps=0.1, tag=\"ADVUNI\",\n",
    "                       eval_loader=None, patience=6, min_delta=0.0):\n",
    "    model = copy.deepcopy(teacher_model)\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    U = torch.full((N_CLASSES,), 1.0 / N_CLASSES)\n",
    "\n",
    "    def _one_epoch(ep):\n",
    "        model.train()\n",
    "        for (xb_r, yb_r), (xb_f, _) in zip(retain_loader, forget_loader):\n",
    "            x_adv = fgsm_on_inputs(model, xb_f, eps=eps)\n",
    "            opt.zero_grad()\n",
    "            Lr = ce(model(xb_r), yb_r)\n",
    "            pf = torch.softmax(model(x_adv), dim=1)\n",
    "            L_u = torch.sum(pf * (torch.log(pf + 1e-8) - torch.log(U.to(pf.device))), dim=1).mean()\n",
    "            (Lr + lam * L_u).backward()\n",
    "            opt.step()\n",
    "\n",
    "    model, _ = train_with_es(model, epochs, eval_loader, tag, _one_epoch,\n",
    "                             best_is_max=True, patience=patience, min_delta=min_delta)\n",
    "    return model\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluation wrapper\n",
    "# ----------------------------\n",
    "def evaluate_suite(method_name, model_unlearned, model_teacher, model_oracle,\n",
    "                   loaders_dict, scenario_tag, csv_rows: List[Dict]):\n",
    "    retain_loader = loaders_dict[\"retain\"]\n",
    "    forget_loader = loaders_dict[\"forget\"]\n",
    "    test_loader = loaders_dict[\"test\"]\n",
    "\n",
    "    acc_r_u = evaluate_acc(model_unlearned, retain_loader)\n",
    "    acc_f_u = evaluate_acc(model_unlearned, forget_loader)\n",
    "    acc_t_u = evaluate_acc(model_unlearned, test_loader)\n",
    "\n",
    "    acc_r_o = evaluate_acc(model_teacher, retain_loader)\n",
    "    acc_f_o = evaluate_acc(model_teacher, forget_loader)\n",
    "    acc_t_o = evaluate_acc(model_teacher, test_loader)\n",
    "\n",
    "    forget_drop = acc_f_o - acc_f_u\n",
    "    retain_drop = acc_r_o - acc_r_u\n",
    "    test_drop   = acc_t_o - acc_t_u\n",
    "\n",
    "    probs_u_r, _ = get_probs(model_unlearned, retain_loader)\n",
    "    probs_o_r, _ = get_probs(model_oracle,   retain_loader)\n",
    "\n",
    "    probs_u_t, _ = get_probs(model_unlearned, test_loader)\n",
    "    probs_o_t, _ = get_probs(model_oracle,   test_loader)\n",
    "\n",
    "    kl_r = kl_divergence(probs_u_r, probs_o_r)\n",
    "    js_r = js_divergence(probs_u_r, probs_o_r)\n",
    "    kl_t = kl_divergence(probs_u_t, probs_o_t)\n",
    "    js_t = js_divergence(probs_u_t, probs_o_t)\n",
    "\n",
    "    agree_r = agreement_rate(model_unlearned, model_oracle, retain_loader)\n",
    "    agree_t = agreement_rate(model_unlearned, model_oracle, test_loader)\n",
    "\n",
    "    mia_auc = None\n",
    "    if \"mia_nonmembers\" in loaders_dict and loaders_dict[\"mia_nonmembers\"] is not None:\n",
    "        mia_auc = mia_auc_confidence(model_unlearned, forget_loader, loaders_dict[\"mia_nonmembers\"])\n",
    "\n",
    "    uqi = forget_drop - 0.5 * (retain_drop + test_drop)\n",
    "\n",
    "    row = {\n",
    "        \"scenario\": scenario_tag,\n",
    "        \"method\": method_name,\n",
    "        \"acc_retain_unlearn\": acc_r_u,\n",
    "        \"acc_forget_unlearn\": acc_f_u,\n",
    "        \"acc_test_unlearn\": acc_t_u,\n",
    "        \"acc_retain_orig\": acc_r_o,\n",
    "        \"acc_forget_orig\": acc_f_o,\n",
    "        \"acc_test_orig\": acc_t_o,\n",
    "        \"forget_drop\": forget_drop,\n",
    "        \"retain_drop\": retain_drop,\n",
    "        \"test_drop\": test_drop,\n",
    "        \"UQI\": uqi,\n",
    "        \"KL_retain\": kl_r,\n",
    "        \"JS_retain\": js_r,\n",
    "        \"KL_test\": kl_t,\n",
    "        \"JS_test\": js_t,\n",
    "        \"Agree_retain\": agree_r,\n",
    "        \"Agree_test\": agree_t,\n",
    "        \"MIA_AUC\": mia_auc\n",
    "    }\n",
    "    csv_rows.append(row)\n",
    "    print(f\"[{scenario_tag} | {method_name}] \"\n",
    "          f\"Ret {acc_r_u:.3f} For {acc_f_u:.3f} Test {acc_t_u:.3f} | \"\n",
    "          f\"UQI {uqi:.3f} | KLr {kl_r:.3f} JSt {js_t:.3f} | AgreeT {agree_t:.3f} | MIA {mia_auc}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Main experiment runner\n",
    "# ----------------------------\n",
    "def run_experiment():\n",
    "    # 1) Data\n",
    "    if USE_CNN_FRONTEND:\n",
    "        # Fashion-MNIST as images [N,1,28,28]\n",
    "        X, y = load_data_cnn(samples_per_class=800)\n",
    "    else:\n",
    "        # Fashion-MNIST PCA → N_QUBITS\n",
    "        X, y = load_data(n_components=N_QUBITS, samples_per_class=100)\n",
    "\n",
    "    (X_tr_all, y_tr_all, X_te, y_te), (X_dr_A, y_dr_A, X_df_A, y_df_A), (X_dr_B, y_dr_B, X_df_B, y_df_B) = \\\n",
    "        split_scenarios(X, y, subset_rate=0.02)\n",
    "\n",
    "    # 2) Teacher with Early Stopping\n",
    "    teacher, (train_loader_full, test_loader_global), teacher_path = train_teacher(\n",
    "        X_train=X_tr_all, y_train=y_tr_all, X_test=X_te, y_test=y_te, tag=\"teacher_full\",\n",
    "        patience=8, min_delta=0.0\n",
    "    )\n",
    "\n",
    "    # 3) Scenario-specific loaders\n",
    "    retain_loader_A = DataLoader(TensorDataset(torch.tensor(X_dr_A, dtype=torch.float32),\n",
    "                                               torch.tensor(y_dr_A, dtype=torch.long)), batch_size=BATCH, shuffle=True)\n",
    "    forget_loader_A = DataLoader(TensorDataset(torch.tensor(X_df_A, dtype=torch.float32),\n",
    "                                               torch.tensor(y_df_A, dtype=torch.long)), batch_size=BATCH, shuffle=True)\n",
    "\n",
    "    test_loader = DataLoader(TensorDataset(torch.tensor(X_te, dtype=torch.float32),\n",
    "                                           torch.tensor(y_te, dtype=torch.long)), batch_size=BATCH, shuffle=False)\n",
    "\n",
    "    # MIA AUC non-members for scenario A\n",
    "    if len(X_df_A) > 0:\n",
    "        idx = np.random.choice(len(X_dr_A), size=len(X_df_A), replace=False)\n",
    "        X_nm = X_dr_A[idx]; y_nm = y_dr_A[idx]\n",
    "        mia_nonmembers_A = DataLoader(TensorDataset(torch.tensor(X_nm, dtype=torch.float32),\n",
    "                                                    torch.tensor(y_nm, dtype=torch.long)), batch_size=BATCH, shuffle=False)\n",
    "    else:\n",
    "        mia_nonmembers_A = None\n",
    "\n",
    "    # Scenario B\n",
    "    retain_loader_B = DataLoader(TensorDataset(torch.tensor(X_dr_B, dtype=torch.float32),\n",
    "                                               torch.tensor(y_dr_B, dtype=torch.long)), batch_size=BATCH, shuffle=True)\n",
    "    forget_loader_B = DataLoader(TensorDataset(torch.tensor(X_df_B, dtype=torch.float32),\n",
    "                                               torch.tensor(y_df_B, dtype=torch.long)), batch_size=BATCH, shuffle=True)\n",
    "\n",
    "    # 4) Oracles per scenario (retrain without Df) with ES\n",
    "    oracle_A, _, _ = train_teacher(X_train=X_dr_A, y_train=y_dr_A, X_test=X_te, y_test=y_te, tag=\"oracle_subsetA\",\n",
    "                                   patience=8, min_delta=0.0)\n",
    "    oracle_B, _, _ = train_teacher(X_train=X_dr_B, y_train=y_dr_B, X_test=X_te, y_test=y_te, tag=\"oracle_fullclassB\",\n",
    "                                   patience=8, min_delta=0.0)\n",
    "\n",
    "    results_rows = []\n",
    "\n",
    "    # ---------- Scenario A: 2% subset forget ----------\n",
    "    loaders_A = {\"retain\": retain_loader_A, \"forget\": forget_loader_A, \"test\": test_loader, \"mia_nonmembers\": mia_nonmembers_A}\n",
    "\n",
    "    ga_A = method_gradient_ascent(teacher, forget_loader_A, epochs=25, tag=\"GA_A\",\n",
    "                                  eval_loader=test_loader, patience=6)\n",
    "    evaluate_suite(\"GA\", ga_A, teacher, oracle_A, loaders_A, \"subset2pct\", results_rows)\n",
    "\n",
    "    fisher_A = method_fisher_unlearning(teacher, forget_loader_A, epochs=25, tag=\"Fisher_A\",\n",
    "                                        eval_loader=test_loader, patience=6)\n",
    "    evaluate_suite(\"Fisher\", fisher_A, teacher, oracle_A, loaders_A, \"subset2pct\", results_rows)\n",
    "\n",
    "    neg_A = method_neggrad_plus(teacher, retain_loader_A, forget_loader_A, epochs=25, tag=\"NegGradPlus_A\",\n",
    "                                eval_loader=test_loader, patience=6)\n",
    "    evaluate_suite(\"NegGradPlus\", neg_A, teacher, oracle_A, loaders_A, \"subset2pct\", results_rows)\n",
    "\n",
    "    cf1_A = method_cf_k(teacher, forget_loader_A, k=1, epochs=25, tag=\"CFk_A\",\n",
    "                        eval_loader=test_loader, patience=6)\n",
    "    evaluate_suite(\"CF-k1\", cf1_A, teacher, oracle_A, loaders_A, \"subset2pct\", results_rows)\n",
    "\n",
    "    eu1_A = method_eu_k(teacher, retain_loader_A, k=1, epochs=25, tag=\"EUk_A\",\n",
    "                        eval_loader=test_loader, patience=6)\n",
    "    evaluate_suite(\"EU-k1\", eu1_A, teacher, oracle_A, loaders_A, \"subset2pct\", results_rows)\n",
    "\n",
    "    scrub_A_simple = method_scrub(teacher, retain_loader_A, forget_loader_A,\n",
    "                                  epochs=60, lam_r=1.0, lam_f=1.5, tag=\"SCRUB_A_simple\",\n",
    "                                  rewind=False, eval_loader=test_loader, patience=8)\n",
    "    evaluate_suite(\"SCRUB\", scrub_A_simple, teacher, oracle_A, loaders_A, \"subset2pct\", results_rows)\n",
    "\n",
    "    scrub_A = method_scrub(teacher, retain_loader_A, forget_loader_A, epochs=60, lam_r=1.0, lam_f=1.5, tag=\"SCRUB_A\",\n",
    "                           rewind=True, eval_loader=test_loader, patience=8)\n",
    "    evaluate_suite(\"SCRUB(+R)\", scrub_A, teacher, oracle_A, loaders_A, \"subset2pct\", results_rows)\n",
    "\n",
    "    cert_A = method_certified_unlearning(teacher, retain_loader_A,\n",
    "                                         epochs=12, sigma=0.05, clip_norm=1.0, tag=\"Certified_A\",\n",
    "                                         eval_loader=test_loader, patience=6)\n",
    "    evaluate_suite(\"Certified\", cert_A, teacher, oracle_A, loaders_A, \"subset2pct\", results_rows)\n",
    "\n",
    "    qmul_A = method_qmul(teacher, retain_loader_A, forget_loader_A,\n",
    "                         epochs=12, alpha=1.0, tag=\"Q-MUL_A\", eval_loader=test_loader, patience=6)\n",
    "    evaluate_suite(\"Q-MUL\", qmul_A, teacher, oracle_A, loaders_A, \"subset2pct\", results_rows)\n",
    "\n",
    "    lca_A = method_label_complement_augmentation(teacher, retain_loader_A, forget_loader_A,\n",
    "                                                 epochs=25, beta=1.0, tag=\"LCA_A\",\n",
    "                                                 eval_loader=test_loader, patience=6)\n",
    "    evaluate_suite(\"LCA\", lca_A, teacher, oracle_A, loaders_A, \"subset2pct\", results_rows)\n",
    "\n",
    "    adv_A = method_adv_uniform(teacher, retain_loader_A, forget_loader_A, epochs=15, eps=0.1, lam=1.0, tag=\"ADVUNI_A\",\n",
    "                               eval_loader=test_loader, patience=6)\n",
    "    evaluate_suite(\"ADV-UNIFORM\", adv_A, teacher, oracle_A, loaders_A, \"subset2pct\", results_rows)\n",
    "\n",
    "    # ---------- Scenario B: full-class forget ----------\n",
    "    loaders_B = {\"retain\": retain_loader_B, \"forget\": forget_loader_B, \"test\": test_loader}\n",
    "\n",
    "    ga_B = method_gradient_ascent(teacher, forget_loader_B, epochs=25, tag=\"GA_B\",\n",
    "                                  eval_loader=test_loader, patience=6)\n",
    "    evaluate_suite(\"GA\", ga_B, teacher, oracle_B, loaders_B, \"fullclass\", results_rows)\n",
    "\n",
    "    fisher_B = method_fisher_unlearning(teacher, forget_loader_B, epochs=25, tag=\"Fisher_B\",\n",
    "                                        eval_loader=test_loader, patience=6)\n",
    "    evaluate_suite(\"Fisher\", fisher_B, teacher, oracle_B, loaders_B, \"fullclass\", results_rows)\n",
    "\n",
    "    neg_B = method_neggrad_plus(teacher, retain_loader_B, forget_loader_B, epochs=25, tag=\"NegGradPlus_B\",\n",
    "                                eval_loader=test_loader, patience=6)\n",
    "    evaluate_suite(\"NegGradPlus\", neg_B, teacher, oracle_B, loaders_B, \"fullclass\", results_rows)\n",
    "\n",
    "    cf1_B = method_cf_k(teacher, forget_loader_B, k=1, epochs=25, tag=\"CFk_B\",\n",
    "                        eval_loader=test_loader, patience=6)\n",
    "    evaluate_suite(\"CF-k1\", cf1_B, teacher, oracle_B, loaders_B, \"fullclass\", results_rows)\n",
    "\n",
    "    eu1_B = method_eu_k(teacher, retain_loader_B, k=1, epochs=25, tag=\"EUk_B\",\n",
    "                        eval_loader=test_loader, patience=6)\n",
    "    evaluate_suite(\"EU-k1\", eu1_B, teacher, oracle_B, loaders_B, \"fullclass\", results_rows)\n",
    "\n",
    "    scrub_B_simple = method_scrub(teacher, retain_loader_B, forget_loader_B,\n",
    "                                  epochs=60, lam_r=1.0, lam_f=1.5, tag=\"SCRUB_B_simple\",\n",
    "                                  rewind=False, eval_loader=test_loader, patience=8)\n",
    "    evaluate_suite(\"SCRUB\", scrub_B_simple, teacher, oracle_B, loaders_B, \"fullclass\", results_rows)\n",
    "\n",
    "    scrub_B = method_scrub(teacher, retain_loader_B, forget_loader_B, epochs=60, lam_r=1.0, lam_f=1.5, tag=\"SCRUB_B\",\n",
    "                           rewind=True, eval_loader=test_loader, patience=8)\n",
    "    evaluate_suite(\"SCRUB(+R)\", scrub_B, teacher, oracle_B, loaders_B, \"fullclass\", results_rows)\n",
    "\n",
    "    cert_B = method_certified_unlearning(teacher, retain_loader_B,\n",
    "                                         epochs=12, sigma=0.05, clip_norm=1.0, tag=\"Certified_B\",\n",
    "                                         eval_loader=test_loader, patience=6)\n",
    "    evaluate_suite(\"Certified\", cert_B, teacher, oracle_B, loaders_B, \"fullclass\", results_rows)\n",
    "\n",
    "    qmul_B = method_qmul(teacher, retain_loader_B, forget_loader_B,\n",
    "                         epochs=12, alpha=1.0, tag=\"Q-MUL_B\", eval_loader=test_loader, patience=6)\n",
    "    evaluate_suite(\"Q-MUL\", qmul_B, teacher, oracle_B, loaders_B, \"fullclass\", results_rows)\n",
    "\n",
    "    lca_B = method_label_complement_augmentation(teacher, retain_loader_B, forget_loader_B,\n",
    "                                                 epochs=25, beta=1.0, tag=\"LCA_B\",\n",
    "                                                 eval_loader=test_loader, patience=6)\n",
    "    evaluate_suite(\"LCA\", lca_B, teacher, oracle_B, loaders_B, \"fullclass\", results_rows)\n",
    "\n",
    "    adv_B = method_adv_uniform(teacher, retain_loader_B, forget_loader_B, epochs=15, eps=0.1, lam=1.0, tag=\"ADVUNI_B\",\n",
    "                               eval_loader=test_loader, patience=6)\n",
    "    evaluate_suite(\"ADV-UNIFORM\", adv_B, teacher, oracle_B, loaders_B, \"fullclass\", results_rows)\n",
    "\n",
    "    # Save results CSV\n",
    "    df = pd.DataFrame(results_rows)\n",
    "    csv_path = os.path.join(OUT_DIR, \"results_summary.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nSaved results CSV to: {csv_path}\")\n",
    "\n",
    "    # Simple plots\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        for scenario in df[\"scenario\"].unique():\n",
    "            d = df[df[\"scenario\"] == scenario]\n",
    "            plt.figure(figsize=(10,5))\n",
    "            for metric in [\"acc_test_unlearn\", \"UQI\", \"Agree_test\"]:\n",
    "                if metric in d.columns:\n",
    "                    plt.plot(d[\"method\"], d[metric], marker=\"o\", label=metric)\n",
    "            plt.title(f\"Scenario: {scenario}\")\n",
    "            plt.xticks(rotation=30)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plot_path = os.path.join(OUT_DIR, f\"plot_{scenario}.png\")\n",
    "            plt.savefig(plot_path)\n",
    "            print(f\"Saved plot: {plot_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Plotting failed:\", e)\n",
    "\n",
    "# ----------------------------\n",
    "# Run\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4782da21-e71f-4d10-bc0b-029f691dec7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
